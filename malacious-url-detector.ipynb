{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4864816",
   "metadata": {},
   "source": [
    "## Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from joblib import dump, load\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339559d",
   "metadata": {},
   "source": [
    "## Creating all the possible combinations of n-grams using lowercase alphabets and digits and storing in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold=np.inf)\n",
    "alphanum = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z','0','1','2','3','4','5','6','7','8','9']\n",
    "permutations = itertools.product(alphanum, repeat=3)\n",
    "featuresDict = {}\n",
    "counter = 0\n",
    "for perm in permutations:\n",
    "    #print(perm)\n",
    "    f=''\n",
    "    for char in perm:\n",
    "        f = f+char;\n",
    "    featuresDict[(''.join(perm))] = counter\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f415361",
   "metadata": {},
   "source": [
    "## A function that takes a sentence as imput and returns list of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66387180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram(sentence):\n",
    "    s = sentence.lower()\n",
    "    s = ''.join(e for e in s if e.isalnum()) #replace spaces and slashes\n",
    "    processedList = []\n",
    "    for tup in list(ngrams(s,3)):\n",
    "        processedList.append((''.join(tup)))\n",
    "    return processedList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4a449",
   "metadata": {},
   "source": [
    "## A function that accepts a dataframe and 2 arrays X & y. X denotes the features that will be used for training and y is the label \n",
    "## URL is then stripped down to the suitable format for pre-processing like removing unwanted Dots, Slash , WWW. , .com etc.\n",
    "## This data is then transferred to the pre-processing module for conversion into suitable format for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66869978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(dataframe, X, y):\n",
    "    #print(dataframe)\n",
    "    for index,row in df.iterrows():\n",
    "        url = row['url'].strip().replace(\"https://\",\"\")\n",
    "        url = row['url'].strip().replace(\"http://\",\"\")\n",
    "        url = url.replace(\"http://\",\"\")\n",
    "        url = re.sub(r'\\.[A-Za-z0-9]+/*','',url)\n",
    "        for gram in generate_ngram(url):\n",
    "            try:\n",
    "                X[index][featuresDict[gram]] = X[index][featuresDict[gram]] + 1\n",
    "            except:\n",
    "                print(gram,\"doesn't exist\")\n",
    "        y[index] = int(row['label'])\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "id": "38e7f999",
   "metadata": {},
   "source": [
    "## Load the dataset and split it into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34185c",
   "metadata": {},
   "source": [
    "url_dataframe = pd.read_csv(\"malicious-url-detector_dataset.csv\")\n",
    "train_df, test_df = train_test_split(url_dataframe, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452ecf0",
   "metadata": {},
   "source": [
    "## The dataset has 1 million rows. \n",
    "## we will be using SGDClassifier which by default fits a linear support vector machine (SVM).\n",
    "## This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the partial_fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rows = 25000\n",
    "no_of_batches = int(train_df.shape[0]/no_of_rows) +1\n",
    "classifier = SGDClassifier()\n",
    "#print(no_of_batches)\n",
    "for i in range(0, no_of_batches):\n",
    "    start = no_of_rows*i\n",
    "    if start + no_of_rows > train_df.shape[0] :\n",
    "        df = train_df.iloc[start:,:]\n",
    "    else :\n",
    "        df = train_df.iloc[start:start+no_of_rows, :]\n",
    "    df = df.reset_index()\n",
    "    (X,y) = preprocess_sentences(df, \\\n",
    "                                 np.zeros([df.shape[0], 46656],dtype=\"int\"), \\\n",
    "                                 np.zeros(df.shape[0],dtype=\"int\"))\n",
    "    classifier.partial_fit(X, y, classes=np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc58f50",
   "metadata": {},
   "source": [
    "## Testing the model against the test set and getting the count of correct and incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33688539",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rows = 25000\n",
    "no_of_batches = int(test_df.shape[0]/no_of_rows) +1\n",
    "#print(no_of_batches)\n",
    "correct = 0;\n",
    "incorrect = 0\n",
    "for i in range(0, no_of_batches):\n",
    "    start = no_of_rows*i\n",
    "    if start + no_of_rows > train_df.shape[0] :\n",
    "        df = train_df.iloc[start:,:]\n",
    "    else :\n",
    "        df = test_df.iloc[start:start+no_of_rows, :]\n",
    "    df = df.reset_index()\n",
    "    (X_test,y_test) = preprocess_sentences(df, \\\n",
    "                                 np.zeros([df.shape[0], 46656],dtype=\"int\"), \\\n",
    "                                 np.zeros(df.shape[0],dtype=\"int\"))\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    for index,row in df.iterrows():\n",
    "        if row['label'] == y_pred[index]:\n",
    "            correct = correct+1\n",
    "        else:\n",
    "            incorrect = incorrect + 1        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d148c",
   "metadata": {},
   "source": [
    "## Displaying the result of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct Predictions \", correct)\n",
    "print(\"Incorrect Predictions \", incorrect)\n",
    "accuracy = (correct/test_df.shape[0])*100\n",
    "print(\"Accuracy of the model is: \"'{0:.4g}'.format(accuracy), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
